{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUxD4gOd-AuO"
      },
      "source": [
        "## Setup: Install and Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyH_VgjW7Bp3"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJm7y6GH3WyB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "# Automatic differentiation library enables efficient calculation of gradients for numerical functions.\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "#building and training graph neural networks.\n",
        "import flax\n",
        "# a high-level API for building neural networks.\n",
        "import haiku as hk\n",
        "# to define neural network models\n",
        "import optax\n",
        "# optimization\n",
        "import pickle\n",
        "#serialize and deserialize\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK0rGWf56-Uq"
      },
      "outputs": [],
      "source": [
        "def build_toy_graph() -> jraph.GraphsTuple:\n",
        "  #  returns a jraph.GraphsTuple object that represents a toy graph\n",
        "\n",
        "  # Nodes are defined implicitly by their features.\n",
        "  # We will add four nodes, each with a feature, e.g.\n",
        "  # node 0 has feature [0.],\n",
        "  # node 1 has featre [2.] etc.\n",
        "  # len(node_features) is the number of nodes.\n",
        "  node_features = jnp.array([[0.], [2.], [4.], [6.]])\n",
        "\n",
        "  # We will now specify 5 directed edges connecting the nodes we defined above.\n",
        "  # We define this with `senders` (source node indices) and `receivers`\n",
        "  # (destination node indices).\n",
        "  # For example, to add an edge from node 0 to node 1, we append 0 to senders,\n",
        "  # and 1 to receivers.\n",
        "  # We can do the same for all 5 edges:\n",
        "  # 0 -> 1\n",
        "  # 1 -> 2\n",
        "  # 2 -> 0\n",
        "  # 3 -> 0\n",
        "  # 0 -> 3\n",
        "  senders = jnp.array([0, 1, 2, 3, 0])\n",
        "  receivers = jnp.array([1, 2, 0, 0, 3])\n",
        "\n",
        "  # You can optionally add edge attributes to the 5 edges.\n",
        "  edges = jnp.array([[5.], [6.], [7.], [8.], [8.]])\n",
        "\n",
        "  # We then save the number of nodes and the number of edges.\n",
        "  # This information is used to make running GNNs over multiple graphs\n",
        "  # in a GraphsTuple possible.\n",
        "  n_node = jnp.array([4])\n",
        "  n_edge = jnp.array([5])\n",
        "\n",
        "  # Optionally you can add `global` information, such as a graph label.\n",
        "  global_context = jnp.array([[1]]) # Same feature dims as nodes and edges.\n",
        "  graph = jraph.GraphsTuple(\n",
        "      nodes=node_features,\n",
        "      edges=edges,\n",
        "      senders=senders,\n",
        "      receivers=receivers,\n",
        "      n_node=n_node,\n",
        "      n_edge=n_edge,\n",
        "      globals=global_context\n",
        "      )\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Jtg_y-TqCW"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWGPulK_tYE"
      },
      "source": [
        "#### Inspecting the GraphsTuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjfUYT_UzoQ9"
      },
      "outputs": [],
      "source": [
        "# Number of nodes\n",
        "# Note that `n_node` returns an array. The length of `n_node` corresponds to\n",
        "# the number of graphs stored in one `GraphsTuple`.\n",
        "# In this case, we only have one graph, so n_node has length 1.\n",
        "graph.n_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQLF-mfOzXGK"
      },
      "outputs": [],
      "source": [
        "# Number of edges\n",
        "graph.n_edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdzVSy04zp3-"
      },
      "outputs": [],
      "source": [
        "# Node features\n",
        "graph.nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9_VpQSZzua3"
      },
      "outputs": [],
      "source": [
        "# Edge features\n",
        "graph.edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvN0pbg0z8Ir"
      },
      "outputs": [],
      "source": [
        "# Edges\n",
        "graph.senders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNI4PVR-z-HL"
      },
      "outputs": [],
      "source": [
        "graph.receivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayThRCYpz4wj"
      },
      "outputs": [],
      "source": [
        "# Graph-level features\n",
        "graph.globals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Pwh9e7d8gN"
      },
      "source": [
        "#### Visualizing the Graph\n",
        "To visualize the graph structure of the graph we created above, we will use the networkx library because it already has functions for drawing graphs.\n",
        "\n",
        "We first convert the `jraph.GraphsTuple` to a `networkx.DiGraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7q5ySSmVL3x"
      },
      "outputs": [],
      "source": [
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
        "#  jraph.GraphsTuple object as input and converts it to a networkx Graph object.\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  # reates a new DiGraph object from the networkx library.\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      # jraph_graph object is None\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph\n",
        "\n",
        "\n",
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNK5SeajWQWO"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc6fCm0zzOv-"
      },
      "source": [
        "### Simple GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc7u-eHvg5Zp"
      },
      "outputs": [],
      "source": [
        "def apply_simplified_gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  # Unpack GraphsTuple\n",
        "  nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "  # 1. Update node features\n",
        "  # For simplicity, we will first use an identify function here, and replace it\n",
        " \n",
        "  update_node_fn = lambda nodes: nodes\n",
        "  nodes = update_node_fn(nodes)\n",
        "\n",
        "  # 2. Aggregate node features over nodes in neighborhood\n",
        "  # Equivalent to jnp.sum(n_node), but jittable\n",
        "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "  aggregate_nodes_fn = jax.ops.segment_sum\n",
        "\n",
        "  # Compute new node features by aggregating messages from neighboring nodes\n",
        "  nodes = tree.tree_map(lambda x: aggregate_nodes_fn(x[senders], receivers,\n",
        "                                        total_num_nodes), nodes)\n",
        "  out_graph = graph._replace(nodes=nodes)\n",
        "  return out_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs66eJ1ZawD2"
      },
      "source": [
        "We can now run the graph convolution on our toy graph from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhG3r8P-aiBb"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHJ_CYthY8P"
      },
      "source": [
        "Here is the visualized graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsaiSIZYbPzw"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAwAgy2bB1P"
      },
      "outputs": [],
      "source": [
        "out_graph = apply_simplified_gcn(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ryPhwtcGTx"
      },
      "source": [
        "Since we used the identity function for updating nodes and sum aggregation, we can verify the results pretty easily. As a reminder, in this toy graph, the node features are the same as the node index.\n",
        "\n",
        "Node 0: sum of features from node 2 and node 3 $\\rightarrow$ 10.\n",
        "\n",
        "Node 1: sum of features from node 0 $\\rightarrow$ 0.\n",
        "\n",
        "Node 2: sum of features from node 1 $\\rightarrow$ 2.\n",
        "\n",
        "Node 3: sum of features from node 0 $\\rightarrow$ 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwEtzddrbHri"
      },
      "outputs": [],
      "source": [
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rt3AB2tvv1o"
      },
      "source": [
        "### Add Trainable Parameters to GCN layer\n",
        "So far our graph convolution operation doesn't have any learnable parameters.\n",
        "Let's add an MLP block to the update function to make it trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK9W9J4GsDbC"
      },
      "outputs": [],
      "source": [
        "class MLP(hk.Module):\n",
        "  def __init__(self, features: jnp.ndarray):\n",
        "    super().__init__()\n",
        "    self.features = features\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    layers = []\n",
        "    for feat in self.features[:-1]:\n",
        "      layers.append(hk.Linear(feat))\n",
        "      layers.append(jax.nn.relu)\n",
        "    layers.append(hk.Linear(self.features[-1]))\n",
        "\n",
        "    mlp = hk.Sequential(layers)\n",
        "    return mlp(x)\n",
        "\n",
        "# Use MLP block to define the update node function\n",
        "update_node_fn = lambda x: MLP(features=[8, 4])(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQCaukgyOue"
      },
      "source": [
        "#### Check outputs of `update_node_fn` with MLP Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ImWUoi9s2-x"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BCg7cYwyBRw"
      },
      "outputs": [],
      "source": [
        "update_node_module = hk.without_apply_rng(hk.transform(update_node_fn))\n",
        "params = update_node_module.init(jax.random.PRNGKey(42), graph.nodes)\n",
        "out = update_node_module.apply(params, graph.nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUu13gBzm2S"
      },
      "source": [
        "As output, we expect the updated node features. We should see one array of dim 4 for each of the 4 nodes, which is the result of applying a single MLP block to the features of each node individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PblLCX67ubZ-"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD1Qc7xK-SkI"
      },
      "source": [
        "#### Add Self-Edges (Edges connecting a node to itself)\n",
        "For each node, add an edge of the node onto itself. This way, nodes will include themselves in the aggregation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8LO_UMN-ReR"
      },
      "outputs": [],
      "source": [
        "def add_self_edges_fn(receivers: jnp.ndarray, senders: jnp.ndarray,\n",
        "                      total_num_nodes: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
        "  receivers = jnp.concatenate((receivers, jnp.arange(total_num_nodes)), axis=0)\n",
        "  senders = jnp.concatenate((senders, jnp.arange(total_num_nodes)), axis=0)\n",
        "  return receivers, senders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RCGC0XbkjCV"
      },
      "source": [
        "#### Add Symmetric Normalization\n",
        "\n",
        "Note that the nodes may have different numbers of neighbors / degrees.\n",
        "This could lead to instabilities during neural network training, e.g. exploding or vanishing gradients. To address that, normalization is a commonly used method. In this case, we will normalize by node degrees.\n",
        "\n",
        "As a first attempt, we could count the number of incoming edges (including self-edge) and divide by that value.\n",
        "\n",
        "More formally, let $A$ be the adjacency matrix defining the edges of the graph.\n",
        "\n",
        "Then we define the degree matrix $D$ as a diagonal matrix with $D_{ii} = \\sum_jA_{ij}$ (the degree of node $i$)\n",
        "\n",
        "\n",
        "Now we can normalize $AH$ by dividing it by the node degrees:\n",
        "$${D}^{-1}AH$$\n",
        "\n",
        "To take both the in and out degrees into account, we can use symmetric normalization, which is also what Kipf and Welling proposed in their [paper](https://arxiv.org/abs/1609.02907):\n",
        "$$D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}H$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxzqrpbj-jZ"
      },
      "source": [
        "### General GCN Layer\n",
        "Now we can write a more general and configurable version of the Graph Convolution layer, allowing the caller to specify:\n",
        "\n",
        "*   **`update_node_fn`**: Function to use to update node features (e.g. the MLP block version we just implemented)\n",
        "*   **`aggregate_nodes_fn`**: Aggregation function to use to aggregate messages from neighbourhood.\n",
        "*  **`add_self_edges`**: Whether to add self edges for aggregation step.\n",
        "* **`symmetric_normalization`**: Whether to add symmetric normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkkOolOVh3nR"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
        "def GraphConvolution(update_node_fn: Callable,\n",
        "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
        "                     add_self_edges: bool = False,\n",
        "                     symmetric_normalization: bool = True) -> Callable:\n",
        "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
        "\n",
        "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
        "  NOTE: This implementation does not add an activation after aggregation.\n",
        "  If you are stacking layers, you may want to add an activation between\n",
        "  each layer.\n",
        "  Args:\n",
        "    update_node_fn: function used to update the nodes. In the paper a single\n",
        "      layer MLP is used.\n",
        "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
        "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
        "      paper definition of GCN. Defaults to False.\n",
        "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
        "      True.\n",
        "\n",
        "  Returns:\n",
        "    A method that applies a Graph Convolution layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
        "    nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "    # First pass nodes through the node updater.\n",
        "    nodes = update_node_fn(nodes)\n",
        "    # Equivalent to jnp.sum(n_node), but jittable\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
        "      # this case it is not required since a GCN is agnostic to whether\n",
        "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
        "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
        "                                                       total_num_nodes)\n",
        "    else:\n",
        "      conv_senders = senders\n",
        "      conv_receivers = receivers\n",
        "\n",
        "    # pylint: disable=g-long-lambda\n",
        "    if symmetric_normalization:\n",
        "      # Calculate the normalization values.\n",
        "      count_edges = lambda x: jax.ops.segment_sum(\n",
        "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
        "      sender_degree = count_edges(conv_senders)\n",
        "      receiver_degree = count_edges(conv_receivers)\n",
        "\n",
        "      # Pre normalize by sqrt sender degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
        "          nodes,\n",
        "      )\n",
        "      # Aggregate the pre-normalized nodes.\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "      # Post normalize by sqrt receiver degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x:\n",
        "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
        "          nodes,\n",
        "      )\n",
        "    else:\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "    # pylint: enable=g-long-lambda\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  return _ApplyGCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKxcA6XAza33"
      },
      "source": [
        "#### Test General GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TgyCkE9KlUG"
      },
      "outputs": [],
      "source": [
        "gcn_layer = GraphConvolution(\n",
        "    update_node_fn=lambda n: MLP(features=[8, 4])(n),\n",
        "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
        "    add_self_edges=True,\n",
        "    symmetric_normalization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gTPEWvU0DDB"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gcn_layer))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwVE88dTRC6V"
      },
      "source": [
        "## Link prediction on CORA (Citation Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-j7Zq59uRZ"
      },
      "source": [
        "The final problem type we will explore is **link prediction**, an instance of an **edge-level** task. Given a graph, our goal is to predict whether a certain edge $(u,v)$ should be present or not. This is often useful in the recommender system settings (e.g., propose new friends in a social network, propose a movie to a user).\n",
        "\n",
        "As before, the first step is to obtain node latents $h_i$ using a GNN. In this context we will use the autoencoder language and call this GNN **encoder**. Then, we learn a binary classifier $f: (h_i, h_j) \\to z_{i,j}$ (**decoder**), predicting if an edge $(i,j)$ should exist or not. While we could use a more elaborate decoder (e.g., an MLP), a common approach we will also use here is to focus on obtaining good node embeddings, and for the decoder simply use the similarity between node latents, i.e. $z_{i,j} = h_i^T h_j$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfxJdcNCeY7"
      },
      "source": [
        "For this problem we will use the [**Cora** dataset](https://linqs.github.io/linqs-website/datasets/#cora), a citation graph containing 2708 scientific publications. For each publication we have a 1433-dimensional feature vector, which is a bag-of-words representation (with a small, fixed dictionary) of the paper text. The edges in this graph represent citations, and are commonly treated as undirected. Each paper is in one of seven topics (classes) so you can also use this dataset for node classification.\n",
        "\n",
        "Similar to MUTAG, we have converted this dataset to jraph for you.\n",
        "\n",
        "Citation for the use of the Cora dataset:\n",
        "- [Qing Lu and Lise Getoor. Link-Based Classification. International Conference on Machine Learning. 2003.](https://linqs.github.io/linqs-website/publications/#id:lu-icml03)\n",
        "- [Sen, Prithviraj, et al. Collective classification in network data. AI magazine 29.3. 2008.](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "- [Dataset download link](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qo4jSfHtdQ9"
      },
      "outputs": [],
      "source": [
        "# Download jraph version of Cora.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/cora.pickle\n",
        "with open('/tmp/cora.pickle', 'rb') as f:\n",
        "  cora_ds = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lc0O4_h9Bfv"
      },
      "source": [
        "#### Splitting Edges and Adding \"Negative\" Edges\n",
        "For the link prediction task, we split the edges into train, val and test sets and also add \"negative\" examples (edges that do not correspond to a citation). We will ignore the topic classes.\n",
        "\n",
        "For the validation and test splits, we add the same number of existing edges (\"positive examples\") and non-existing edges (\"negative examples\").\n",
        "\n",
        "In contrast to the validation and test splits, the training split only contains positive examples (set $T_+$). The $|T_+|$ negative examples to be used during training will be sampled ad hoc in each epoch and uniformly at random from all edges that are not in $T_+$. This allows the model to see a wider range of negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqZyzwpC3p-u"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split_edges(graph: jraph.GraphsTuple,\n",
        "                               val_perc: float = 0.05,\n",
        "                               test_perc: float = 0.1):\n",
        "  \"\"\"Split edges in input graph into train, val and test splits.\n",
        "\n",
        "  For val and test sets, also include negative edges.\n",
        "  Based on torch_geometric.utils.train_test_split_edges.\n",
        "  \"\"\"\n",
        "  mask = graph.senders < graph.receivers\n",
        "  senders = graph.senders[mask]\n",
        "  receivers = graph.receivers[mask]\n",
        "  num_val = int(val_perc * senders.shape[0])\n",
        "  num_test = int(test_perc * senders.shape[0])\n",
        "  permuted_indices = onp.random.permutation(range(senders.shape[0]))\n",
        "  senders = senders[permuted_indices]\n",
        "  receivers = receivers[permuted_indices]\n",
        "  if graph.edges is not None:\n",
        "    edges = graph.edges[permuted_indices]\n",
        "\n",
        "  val_senders = senders[:num_val]\n",
        "  val_receivers = receivers[:num_val]\n",
        "  if graph.edges is not None:\n",
        "    val_edges = edges[:num_val]\n",
        "\n",
        "  test_senders = senders[num_val:num_val + num_test]\n",
        "  test_receivers = receivers[num_val:num_val + num_test]\n",
        "  if graph.edges is not None:\n",
        "    test_edges = edges[num_val:num_val + num_test]\n",
        "\n",
        "  train_senders = senders[num_val + num_test:]\n",
        "  train_receivers = receivers[num_val + num_test:]\n",
        "  train_edges = None\n",
        "  if graph.edges is not None:\n",
        "    train_edges = edges[num_val + num_test:]\n",
        "\n",
        "  # make training edges undirected by adding reverse edges back in\n",
        "  train_senders_undir = jnp.concatenate((train_senders, train_receivers))\n",
        "  train_receivers_undir = jnp.concatenate((train_receivers, train_senders))\n",
        "  train_senders = train_senders_undir\n",
        "  train_receivers = train_receivers_undir\n",
        "\n",
        "  # Negative edges.\n",
        "  num_nodes = graph.n_node[0]\n",
        "  # Create a negative adjacency mask, s.t. mask[i, j] = True iff edge i->j does\n",
        "  # not exist in the original graph.\n",
        "  neg_adj_mask = onp.ones((num_nodes, num_nodes), dtype=onp.uint8)\n",
        "  # upper triangular part\n",
        "  neg_adj_mask = onp.triu(neg_adj_mask, k=1)\n",
        "  neg_adj_mask[graph.senders, graph.receivers] = 0\n",
        "  neg_adj_mask = neg_adj_mask.astype(onp.bool)\n",
        "  neg_senders, neg_receivers = neg_adj_mask.nonzero()\n",
        "\n",
        "  perm = onp.random.permutation(range(len(neg_senders)))\n",
        "  neg_senders = neg_senders[perm]\n",
        "  neg_receivers = neg_receivers[perm]\n",
        "\n",
        "  val_neg_senders = neg_senders[:num_val]\n",
        "  val_neg_receivers = neg_receivers[:num_val]\n",
        "  test_neg_senders = neg_senders[num_val:num_val + num_test]\n",
        "  test_neg_receivers = neg_receivers[num_val:num_val + num_test]\n",
        "\n",
        "  train_graph = jraph.GraphsTuple(\n",
        "      nodes=graph.nodes,\n",
        "      edges=train_edges,\n",
        "      senders=train_senders,\n",
        "      receivers=train_receivers,\n",
        "      n_node=graph.n_node,\n",
        "      n_edge=jnp.array([len(train_senders)]),\n",
        "      globals=graph.globals)\n",
        "\n",
        "  return train_graph, neg_adj_mask, val_senders, val_receivers, val_neg_senders, val_neg_receivers, test_senders, test_receivers, test_neg_senders, test_neg_receivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cE2CNtN-G3D"
      },
      "source": [
        "#### Test the Edge Splitting Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBruorYJPm6"
      },
      "outputs": [],
      "source": [
        "graph = cora_ds[0]['input_graph']\n",
        "train_graph, neg_adj_mask, val_pos_senders, val_pos_receivers, val_neg_senders, val_neg_receivers, test_pos_senders, test_pos_receivers, test_neg_senders, test_neg_receivers = train_val_test_split_edges(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOMs84n4Gz1e"
      },
      "outputs": [],
      "source": [
        "print(f'Train set: {train_graph.senders.shape[0]} positive edges, we will sample the same number of negative edges at runtime')\n",
        "print(f'Val set: {val_pos_senders.shape[0]} positive edges, {val_neg_senders.shape[0]} negative edges')\n",
        "print(f'Test set: {test_pos_senders.shape[0]} positive edges, {test_neg_senders.shape[0]} negative edges')\n",
        "print(f'Negative adjacency mask shape: {neg_adj_mask.shape}')\n",
        "print(f'Numbe of negative edges to sample from: {neg_adj_mask.sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzwvygx-BS6e"
      },
      "source": [
        "\n",
        "*Note*: It will often happen during training that as a negative example, we sample an initially existing edge (that is now e.g. a positive example in the test set). We are however not allowed to check for this, as we should be unaware of the existence of test edges during training.\n",
        "\n",
        "Assuming our dot product decoder, we are essentially attempting to bring the latents of endpoints of edges from $T_+$ closer together, and make the latents of all other pairs of nodes as distant as possible. As this is impossible to fully satisfy, the hope is that the model will \"fail\" to distance those pairs of nodes where the edges should actually exist (positive examples from the test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGRJAyfLwed"
      },
      "source": [
        "#### Graph Network Model Definition\n",
        "\n",
        "We will use jraph.GraphNetwork to build our graph net model.\n",
        "\n",
        "We first define update functions for node features. We are not using edge or global features for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC3AMCrJLwek"
      },
      "outputs": [],
      "source": [
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.Linear(128), jax.nn.relu, hk.Linear(64)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Network definition.\"\"\"\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn, update_edge_fn=None, update_global_fn=None)\n",
        "  return net(graph)\n",
        "\n",
        "\n",
        "def decode(pred_graph: jraph.GraphsTuple, senders: jnp.ndarray,\n",
        "           receivers: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Given a set of candidate edges, take dot product of respective nodes.\n",
        "\n",
        "  Args:\n",
        "    pred_graph: input graph.\n",
        "    senders: Senders of candidate edges.\n",
        "    receivers: Receivers of candidate edges.\n",
        "\n",
        "  Returns:\n",
        "    For each edge, computes dot product of the features of the two nodes.\n",
        "\n",
        "  \"\"\"\n",
        "  return jnp.squeeze(\n",
        "      jnp.sum(pred_graph.nodes[senders] * pred_graph.nodes[receivers], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmSjMB_TGAUE"
      },
      "source": [
        "To evaluate our model, we first apply the sigmoid function to obtained dot products to get a score $s_{i,j} \\in [0,1]$ for each edge. Now, we can pick a threshold $\\tau$ and say that we predict all pairs $(i,j)$ s.t. $s_{i,j} \\geq \\tau$ as edges (and all the rest as non-edges)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGz7iRGmLwel"
      },
      "source": [
        "#### Loss and ROC-AUC-Metric Function\n",
        "Define the binary classification cross-entropy loss.\n",
        "To aggregate the results over all choices of $\\tau$, we will use ROC-AUC (the area under the ROC curve) as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ld4b3D6Lwel"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_bce_with_logits_loss(x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes binary cross-entropy with logits loss.\n",
        "\n",
        "  Combines sigmoid and BCE, and uses log-sum-exp trick for numerical stability.\n",
        "  See https://stackoverflow.com/a/66909858 if you want to learn more.\n",
        "\n",
        "  Args:\n",
        "    x: Predictions (logits).\n",
        "    y: Labels.\n",
        "\n",
        "  Returns:\n",
        "    Binary cross-entropy loss with mean aggregation.\n",
        "\n",
        "  \"\"\"\n",
        "  max_val = jnp.clip(x, 0, None)\n",
        "  loss = x - x * y + max_val + jnp.log(\n",
        "      jnp.exp(-max_val) + jnp.exp((-x - max_val)))\n",
        "  return loss.mean()\n",
        "\n",
        "\n",
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple,\n",
        "                 senders: jnp.ndarray, receivers: jnp.ndarray,\n",
        "                 labels: jnp.ndarray,\n",
        "                 net: hk.Transformed) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = decode(pred_graph, senders, receivers)\n",
        "  loss = compute_bce_with_logits_loss(preds, labels)\n",
        "  return loss, preds\n",
        "\n",
        "\n",
        "def compute_roc_auc_score(preds: jnp.ndarray,\n",
        "                          labels: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes roc auc (area under the curve) score for classification.\"\"\"\n",
        "  s = jax.nn.sigmoid(preds)\n",
        "  roc_auc = roc_auc_score(labels, s)\n",
        "  return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58D3mm9blJFD"
      },
      "source": [
        "Helper function for sampling negative edges during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTZ4CwVDWrc9"
      },
      "outputs": [],
      "source": [
        "def negative_sampling(\n",
        "    graph: jraph.GraphsTuple, num_neg_samples: int,\n",
        "    key: jnp.DeviceArray) -> Tuple[jnp.DeviceArray, jnp.DeviceArray]:\n",
        "  \"\"\"Samples negative edges, i.e. edges that don't exist in the input graph.\"\"\"\n",
        "  num_nodes = graph.n_node[0]\n",
        "  total_possible_edges = num_nodes**2\n",
        "  # convert 2D edge indices to 1D representation.\n",
        "  pos_idx = graph.senders * num_nodes + graph.receivers\n",
        "\n",
        "  # Percentage to oversample edges, so most likely will sample enough neg edges.\n",
        "  alpha = jnp.abs(1 / (1 - 1.1 *\n",
        "                       (graph.senders.shape[0] / total_possible_edges)))\n",
        "\n",
        "  perm = jax.random.randint(\n",
        "      key,\n",
        "      shape=(int(alpha * num_neg_samples),),\n",
        "      minval=0,\n",
        "      maxval=total_possible_edges,\n",
        "      dtype=jnp.uint32)\n",
        "\n",
        "  # mask where sampled edges are positive edges.\n",
        "  mask = jnp.isin(perm, pos_idx)\n",
        "  # remove positive edges.\n",
        "  perm = perm[~mask][:num_neg_samples]\n",
        "\n",
        "  # convert 1d back to 2d edge indices.\n",
        "  neg_senders = perm // num_nodes\n",
        "  neg_receivers = perm % num_nodes\n",
        "\n",
        "  return neg_senders, neg_receivers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xUW5XkL3YS"
      },
      "source": [
        "Let's write the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjfWFduSESI"
      },
      "outputs": [],
      "source": [
        "def train(dataset: List[Dict[str, Any]], num_epochs: int) -> hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  train_graph, _, val_pos_s, val_pos_r, val_neg_s, val_neg_r, test_pos_s, \\\n",
        "      test_pos_r, test_neg_s, test_neg_r = train_val_test_split_edges(\n",
        "      graph)\n",
        "\n",
        "  # Prepare the validation and test data.\n",
        "  val_senders = jnp.concatenate((val_pos_s, val_neg_s))\n",
        "  val_receivers = jnp.concatenate((val_pos_r, val_neg_r))\n",
        "  val_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(val_pos_s)), jnp.zeros(len(val_neg_s))))\n",
        "  test_senders = jnp.concatenate((test_pos_s, test_neg_s))\n",
        "  test_receivers = jnp.concatenate((test_pos_r, test_neg_r))\n",
        "  test_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(test_pos_s)), jnp.zeros(len(test_neg_s))))\n",
        "  # Initialize the network.\n",
        "  params = net.init(key, train_graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    num_neg_samples = train_graph.senders.shape[0]\n",
        "    train_neg_senders, train_neg_receivers = negative_sampling(\n",
        "        train_graph, num_neg_samples=num_neg_samples, key=key)\n",
        "    train_senders = jnp.concatenate((train_graph.senders, train_neg_senders))\n",
        "    train_receivers = jnp.concatenate(\n",
        "        (train_graph.receivers, train_neg_receivers))\n",
        "    train_labels = jnp.concatenate(\n",
        "        (jnp.ones(len(train_graph.senders)), jnp.zeros(len(train_neg_senders))))\n",
        "\n",
        "    (train_loss,\n",
        "     train_preds), grad = compute_loss_fn(params, train_graph, train_senders,\n",
        "                                          train_receivers, train_labels)\n",
        "\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if epoch % 10 == 0 or epoch == (num_epochs - 1):\n",
        "      train_roc_auc = compute_roc_auc_score(train_preds, train_labels)\n",
        "      val_loss, val_preds = compute_loss(params, train_graph, val_senders,\n",
        "                                         val_receivers, val_labels, net)\n",
        "      val_roc_auc = compute_roc_auc_score(val_preds, val_labels)\n",
        "      print(f'epoch: {epoch}, train_loss: {train_loss:.3f}, '\n",
        "            f'train_roc_auc: {train_roc_auc:.3f}, val_loss: {val_loss:.3f}, '\n",
        "            f'val_roc_auc: {val_roc_auc:.3f}')\n",
        "  test_loss, test_preds = compute_loss(params, train_graph, test_senders,\n",
        "                                       test_receivers, test_labels, net)\n",
        "  test_roc_auc = compute_roc_auc_score(test_preds, test_labels)\n",
        "  print('Training finished')\n",
        "  print(\n",
        "      f'epoch: {epoch}, test_loss: {test_loss:.3f}, test_roc_auc: {test_roc_auc:.3f}'\n",
        "  )\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealPntyvdxyT"
      },
      "source": [
        "Let's train the model! We expect the model to reach roughly test_roc_auc of 0.84.\n",
        "\n",
        "(Note that ROC-AUC is a scalar between 0 and 1, with 1 being the ROC-AUC of a perfect classifier.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LJtk4WKenoE"
      },
      "outputs": [],
      "source": [
        "params = train(cora_ds, num_epochs=200)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}